{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Transformer Lens we can see the contribution from Attention and MLP layers of a transformer block to a particular residual stream.\n",
    "\n",
    "```python\n",
    "def attn(position, block):\n",
    "  return cache[f\"blocks.{block}.hook_attn_out\"][0,position]\n",
    "\n",
    "def mlp(position, block):\n",
    "  return cache[f\"blocks.{block}.hook_mlp_out\"][0,position]\n",
    "```\n",
    "$$\\Delta x^i_j = A + M$$\n",
    "\n",
    "where $A$ is the contribution from Attention layer and $M$ is the contribution from MLP layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention itself (based on softmax of the dot product of the query and key) sets weights on which streams contribute to which, but doesn't directly affect what is actually written to the stream being analysed.\n",
    "\n",
    "If we split heads and freeze attention the attention contribution comes from the QK circuit.\n",
    "\n",
    "We can get the pattern from \n",
    "```python\n",
    "cache[f\"blocks.{block}.hook_mlp_out\"]\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
