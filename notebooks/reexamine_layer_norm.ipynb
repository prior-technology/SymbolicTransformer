{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard LN\n",
    "\n",
    "Include files with vectors copied from transformer lens and set some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "include(\"../data/probe_token.jl\")\n",
    "include(\"../data/pre_norm.jl\")\n",
    "\n",
    "N=512\n",
    "ϵ = 1e-5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ReExaminingLayerNorm.ipynb](https://colab.research.google.com/drive/1S39-w4vzX3VzZx_27X_BtrLs442pOJnJ) (also [described on LessWrong](https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm) ) describes the following as definition for layer-norm from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LN (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "μ(x) = sum(x) / N\n",
    "E(x) = μ(x) \n",
    "c(x) = x .- μ(x)\n",
    "\n",
    "var(x) = sum((x .- μ(x)) .^2 )/N\n",
    "\n",
    "LN(x) = (x .- E(x))/sqrt(var(x) + ϵ) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is equivalent this should return 11.4077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.407851912178797"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bias = 0.8328\n",
    "\n",
    "final_residual = LN(pre_norm)\n",
    "\n",
    "logit = sum(.*(probe_token, final_residual)) + bias\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm from Re-Examining Layer Norm article \n",
    "Following the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u_ϵ (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm(x) = sqrt(sum(x .^ 2))\n",
    "u_ϵ(x) = x .* (1/sqrt(norm(x)^2 + ϵ) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sqrt{n} \\cdot u_{n \\epsilon}(x) = \\frac{x}{\\sqrt{\\textrm{Var}[x] + \\epsilon}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.40785559974425"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_residual = sqrt(512) .* u_ϵ(pre_norm)\n",
    "\n",
    "\n",
    "logit = sum(.*(probe_token, final_residual)) + bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$LN = \\sqrt{n} \\cdot U_{n \\epsilon}(c(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.407851912178797"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_nϵ(x) = x .* (1/sqrt(norm(x)^2 + (512*ϵ)) )\n",
    "\n",
    "\n",
    "final_residual = sqrt(512) .* u_nϵ(c(pre_norm))\n",
    "\n",
    "\n",
    "logit = sum(.*(probe_token, final_residual)) + bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm without direct dependency on basis vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard definition of $mean(v) = \\mu(v)$ is:\n",
    "\n",
    "$$\\mu(v) = \\frac{sum(v)}{N}$$\n",
    "where $sum(v)$ is the sum over components of v, and $N$ is number of components of $v$, which results in a scalar value.\n",
    "\n",
    "This can be stated using the dot product with the vector $\\vec{1} = \\{ 1,1,1,...1 \\}$: $$\\mu(v) = \\frac{<\\vec{1},v>}{N}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centering operation deducts $\\mu(v)$ from each component of $v$:\n",
    "$$c(v) = v - μ(v) \\vec{1}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the earlier definition\n",
    "\n",
    "$$LN = \\sqrt{N}  u_{N \\epsilon}(c(v))$$\n",
    "\n",
    "In terms of Geometric Algebra $<u,v>  = \\frac{1}{2} (uv + vu)$ where uv is the geometric product of vectors $u$ and $v$, so we can rewrite\n",
    "\n",
    "$$c(v) = v - \\frac{<\\vec{1} , v>}{N} \\vec{1}\n",
    "= v - (\\frac{\\vec{1} v + v \\vec{1}}{2N}) \\vec{1}$$\n",
    "\n",
    "Can we go as far as this?\n",
    "$$ = v - \\frac{\\vec{1} v \\vec{1} + v |\\vec{1}|^2}{2N}$$\n",
    "$$ = v - \\frac{\\vec{1}v\\vec{1}}{2N} - \\frac{N}{2N}v$$\n",
    "$$ = \\frac{1}{2}v - \\frac{1}{2N} \\vec{1}v\\vec{1}$$\n",
    "\n",
    "This is checked for N=4 in [center.ipynb](../geometry/center.ipynb)\n",
    "\n",
    "$\\vec{1} v \\vec{1}$ is a multivector - which I believe represents projection through the $\\vec{1}$ vector.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$<c(v), c(v)> = \\frac{1}{4}v^2 - \\frac{1}{4N}v\\vec{1}v\\vec{1} - \\frac{1}{4N}\\vec{1}v\\vec{1}v + \\frac{1}{4N^2} \\vec{1} v \\vec{1} \\vec{1} v \\vec{1}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But $$\\vec{1} v \\vec{1} \\vec{1} v \\vec{1} = N \\vec{1} v v \\vec{1} = N |v|^2 \\vec{1}\\vec{1} = N^2 |v|^2$$ \n",
    "So\n",
    "$$ <c(v), c(v)> = \\frac{1}{4} |v|^2 - \\frac{1}{4N} |v \\vec{1}|^2 - \\frac{1}{4N} |\\vec{1} v |^2 + \\frac{1}{4} |v|^2 $$\n",
    "$$ = \\frac{1}{2} |v|^2 - \\frac{1}{4N} |v \\vec{1}|^2 - \\frac{1}{4N} |\\vec{1} v |^2  $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$u_{\\epsilon}(v) = \\frac{v}{\\sqrt{<v,v> + \\epsilon} }$$\n",
    "\n",
    "$$u_\\epsilon (c(v)) = \\frac{\\frac{1}{2}v - \\frac{1}{2N} \\vec{1}v\\vec{1}}{\\sqrt{\\frac{1}{2} |v|^2 - \\frac{1}{4N} |v \\vec{1}|^2 - \\frac{1}{4N} |\\vec{1} v |^2  + \\epsilon}}$$\n",
    "\n",
    "Low confidence in this - I haven't checked it, and it might simplify to something obvious.\n",
    "\n",
    "Update 2023-07-13 - tested and found wanting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sums and Products\n",
    "\n",
    "We have $u_\\epsilon(v)$ and $c(v)$ expressed in terms of geometric products. Check how they look for scalar products and sums\n",
    "\n",
    "$$c(a+b) = \\frac{1}{2}a + \\frac{1}{2} b - \\frac{1}{2N} \\vec{1}(a+b)\\vec{1}$$\n",
    "\n",
    "Since geometric product is associative;\n",
    "\n",
    "$$c(a+b) = \\frac{1}{2}a + \\frac{1}{2} b - \\frac{1}{2N} \\vec{1}a\\vec{1} - \\frac{1}{2N} \\vec{1}b\\vec{1}$$\n",
    "$$ = \\frac{1}{2}a  - \\frac{1}{2N} \\vec{1}a\\vec{1} + \\frac{1}{2} b  - \\frac{1}{2N} \\vec{1}b\\vec{1}$$\n",
    "$$ = c(a) + c(b)$$\n",
    "\n",
    "$$u_{\\epsilon}(a+b) = \\frac{a+b}{\\sqrt{<a+b,a+b> + \\epsilon} }$$\n",
    "Using $<u,v> = \\frac{1}{2} (uv + vu)$\n",
    "\n",
    "$$<a+b,a+b> = \\frac{1}{2} (a+b)(a+b) + (a+b)(a+b) \\\\\n",
    " = \\frac{1}{2} (a^2 + ab + ba + b^2) + (a^2 + ab + ba + b^2) \\\\\n",
    " = a^2 + ab + ba + b^2$$\n",
    "\n",
    "So $$u_{\\epsilon}(a+b) = \\frac{a+b}{\\sqrt{a^2 + ab + ba + b^2 + \\epsilon} }$$\n",
    "\n",
    "Next, looking at the scalar product\n",
    "$$<x, u_{\\epsilon}(a+b)> = \\frac{1}{2} (x u_{\\epsilon}(a+b) + u_{\\epsilon}(a+b) x) \\\\\n",
    " = \\frac{x (a+b) + (a+b) x}{2 \\sqrt{a^2 + ab + ba + b^2 + \\epsilon} } \\\\\n",
    " = \\frac{x a + x b + a x + b x}{2 \\sqrt{a^2 + ab + ba + b^2 + \\epsilon} } $$\n",
    "\n",
    "The goal is to attribute loss to a or b. \n",
    "$$<x, u_{\\epsilon}(a+b)> = \\frac{x a + a x}{2 \\sqrt{a^2 + ab + ba + b^2 + \\epsilon} } + \\frac{x b + b x}{2 \\sqrt{a^2 + ab + ba + b^2 + \\epsilon} } $$\n",
    "$$= \\frac{<x,a> + <x,b>}{\\sqrt{a^2 + 2<a,b> + b^2 + \\epsilon} } $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Notes\n",
    "\n",
    "Earlier notes on the contribution of two summed vectors to loss under layer normalization, without reference to geometric algebra.\n",
    "\n",
    "$$μ(v) = \\frac{ <v , \\vec{1}> }{N}$$\n",
    "\n",
    "But since $|\\vec{1}| = \\sqrt{N}$  \n",
    "$$μ(v) =  \\frac{ |v| \\sqrt{N} \\cos{\\theta_{v,\\vec{1}}}}{N}   \n",
    "    = \\frac{|v| \\cos{\\theta_{v,\\vec{1}}}}{\\sqrt{N}} $$\n",
    "\n",
    "where $\\theta_{v,\\vec{1}}$ is the angle between $v$ and $\\vec{1}$\n",
    "\n",
    "Applying layer normalization results in a vector of approx unit length, in the direction of the centered vector.\n",
    "\n",
    "$$LN(v) = \\sqrt{N} u_{N \\epsilon}(c(v))$$\n",
    "\n",
    "The inner product between 2 vectors $a$ and $b$, with LN applied to the second\n",
    "$$< a, LN b> \\approx |a| \\cos{\\theta_{a,c(b)}} $$\n",
    "\n",
    "If $b$ is understood as the sum of several vectors, they can be analysed in terms of\n",
    "how they contribute to the angle of the centered vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
