{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Residual Space\n",
    "\n",
    "Logit Lens and Tuned Lens show a representation of the residual space of a transformer based on the unembedding vectors for tokens. Both show that this representation is in some way meaningful beyond the first/last layers. [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) indicates that this representation may approximate the bigram log-likelihood for tokens. \n",
    "\n",
    "The residual space is also shaped by the training goals of attracting attention from later layers, and of contributing to the residual space in later layers. \n",
    "\n",
    "Focussing on the logits of a specific token in a later layer, we can work out how the final residual vector was built up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
